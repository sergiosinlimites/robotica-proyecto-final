<!-- ‚ú¶‚ú¶‚ú¶ PROYECTO FINAL ROB√ìTICA ‚ú¶‚ú¶‚ú¶ -->
<div align="center">
<!-- Banner superior "ne√≥n" -->
<img src="https://capsule-render.vercel.app/api?type=waving&height=200&color=0:04041A,50:14213D,100:0A4D68&text=Proyecto%20Final&fontColor=E0FBFC&fontSize=60&fontAlign=50&fontAlignY=35&desc=Rob√≥tica%20‚Ä¢%20ROS%20‚Ä¢%20Innovaci√≥n%20y%20Tecnolog√≠a&descSize=20&descAlign=50&descAlignY=55" width="100%" />
<br/>

<!-- L√≠nea de texto mecanografiado (animado) -->
<p align="center">
  <img src="https://readme-typing-svg.demolab.com?font=JetBrains+Mono&weight=600&size=20&pause=1200&duration=3500&color=00E5FF&center=true&vCenter=true&width=1000&lines=Proyecto+Final+de+Rob√≥tica+%E2%80%A2+Innovaci√≥n+y+Desarrollo;Control+Avanzado+%E2%80%A2+Simulaci√≥n+%E2%80%A2+Implementaci√≥n" alt="typing">
</p>

---
### üõ∞Ô∏è Descripci√≥n general
Este repositorio contiene el **Proyecto Final** del curso de *Rob√≥tica 2025-II*. 
Incluye el desarrollo completo de un sistema rob√≥tico con control, simulaci√≥n y aplicaci√≥n pr√°ctica.
---
## üßë‚ÄçüöÄ Equipo
<!-- ===== INICIO BLOQUE ANIMACIONES EQUIPO (una animaci√≥n por l√≠nea) ===== -->

<p align="center">
  <img src="https://readme-typing-svg.demolab.com?font=JetBrains+Mono&weight=800&size=22&duration=1000&pause=13000&color=00E5FF&center=true&vCenter=true&width=1000&repeat=true&lines=Integrantes%3A" alt="Integrantes">
</p>
<!-- Nombres aparecen despu√©s del t√≠tulo (delay 1000ms) -->
<p align="center">
  <img src="https://readme-typing-svg.demolab.com?font=JetBrains+Mono&weight=700&size=20&duration=1500&pause=11500&color=7F5AF0&center=true&vCenter=true&width=1000&repeat=true&lines=Sergio+Andr%C3%A9s+Bola%C3%B1os+Penagos+%E2%80%94+sbolanosp%40unal.edu.co" alt="Sergio Andr√©s Bola√±os Penagos">
</p>
<p align="center">
  <img src="https://readme-typing-svg.demolab.com?font=JetBrains+Mono&weight=700&size=20&duration=1500&pause=11500&color=7F5AF0&center=true&vCenter=true&width=1000&repeat=true&lines=Jorge+Nicol%C3%A1s+Garz%C3%B3n+Acevedo+%E2%80%94+jngarzona%40unal.edu.co" alt="Jorge Nicol√°s Garz√≥n Acevedo">
</p>
<p align="center">
  <img src="https://readme-typing-svg.demolab.com?font=JetBrains+Mono&weight=700&size=20&duration=1500&pause=11500&color=7F5AF0&center=true&vCenter=true&width=1000&repeat=true&lines=Sergio+Felipe+Rodriguez+Mayorga+%E2%80%94+sfrodriguezma%40unal.edu.co" alt="Sergio Felipe Rodriguez Mayorga">
</p>
<p align="center">
  <img src="https://readme-typing-svg.demolab.com?font=JetBrains+Mono&weight=700&size=20&duration=1500&pause=11500&color=7F5AF0&center=true&vCenter=true&width=1000&repeat=true&lines=Johan+Camilo+Pati%C3%B1o+Mogoll%C3%B3n+%E2%80%94+jopatinom%40unal.edu.co" alt="Johan Camilo Pati√±o Mogoll√≥n">
</p>

<!-- ===== FIN BLOQUE ANIMACIONES EQUIPO ===== -->
</div>
---


# Proyecto final de rob√≥tica

## 1. Integrantes

- Sergio Andr√©s Bola√±os Penagos
- Sergio Felipe Rodriguez Mayorga
- Johan Camilo Pati√±o Mogoll√≥n
- Jorge Nicolas Garz√≥n Acevedo

## 2. Introducci√≥n y objetivos

Este proyecto implementa un sistema de clasificaci√≥n autom√°tica de objetos usando el robot **Phantom X Pincher** con **ROS 2**, **MoveIt 2** y un pipeline de **visi√≥n por computador** basado en **YOLO**. El robot toma objetos ubicados en una **zona de recolecci√≥n** y los deposita en la **caneca correspondiente** seg√∫n su tipo.

Las figuras consideradas son:

- **Cubo**: Caneca **roja**
- **Cilindro**: Caneca **verde**
- **Pent√°gono**: Caneca **azul**
- **Rect√°ngulo**: Caneca **amarilla**

El sistema est√° compuesto por:

- Un sistema de control con **MoveIt 2** que se encarga de planear y realizar los movimientos tanto del brazo como de la pinza del robot, decidiendo c√≥mo llegar de un punto a otro de forma segura y eficiente.
- Un **nodo de alto nivel** que recibe una **pose objetivo** en el t√≥pico `/pose_command` y ejecuta la trayectoria correspondiente.
- Un **pipeline de visi√≥n** basado en YOLO que detecta la figura presente en la zona de recolecci√≥n usando una c√°mara **ORBBEC Astra Pro**.

**Objetivos principales del proyecto:**

1. Implementar una rutina completa de pick & place para las 4 figuras usando `/pose_command`.
2. Integrar MoveIt 2 con el modelo URDF/Xacro del Phantom X Pincher, incluyendo el entorno de trabajo (base, canecas, m√°stil y canastilla).
3. Entrenar un modelo de YOLO con im√°genes reales capturadas desde la c√°mara y usarlo para detectar las figuras en tiempo real.
4. Integrar la salida de visi√≥n con el nodo de movimiento, logrando una clasificaci√≥n aut√≥noma de objetos en el robot real.

## 3. Instalaci√≥n

Esta secci√≥n resume los pasos principales para instalar dependencias y compilar el workspace. Para detalles completos ver `guias/Setup/readme.md`.

### 3.1 Dependencias generales

- **Sistema operativo recomendado:** Ubuntu 22.04 / 24.04.
- **ROS 2:** Humble o Jazzy (instalaci√≥n desktop completa).
- **Git LFS:** necesario para manejar modelos 3D, videos y archivos pesados.

Instalar Git LFS:

```bash
sudo apt update
sudo apt install git-lfs
git lfs install
```

Instalar los paquetes principales de ROS 2 usados en el proyecto (MoveIt, integraci√≥n con Gazebo, etc.):

```bash
sudo apt-get install ros-humble-ros2-control
sudo apt-get install ros-humble-ros2-controllers
sudo apt-get install ros-humble-xacro
sudo apt-get install ros-humble-ros-gz-*
sudo apt-get install ros-humble-*-ros2-control
sudo apt-get install ros-humble-joint-state-publisher-gui
sudo apt-get install ros-humble-tf-transformations
sudo apt-get install ros-humble-moveit*
```

> Importante: Adaptar el nombre de la distribuci√≥n (`jazzy`/`humble`) seg√∫n el sistema usado.

Por otra parte instalar los siguientes paquetes
```bash
sudo apt-get install python3-pip
pip install transforms3d
pip install flask
pip install pyserial
pip install flask-ask-sdk
pip install ask-sdk
sudo apt install python3-lark
pip install lark
```

Configurar ROS 2 para que siempre quede activo en cada terminal (ejemplo para Humble):

```bash
cd
sudo gedit .bashrc
# Al final del archivo a√±adir
source /opt/ros/humble/setup.bash
```

### 3.2 Clonado del repositorio

```bash
git clone https://github.com/sergiosinlimites/robotica-proyecto-final
cd robotica-proyecto-final/KIT_Phantom_X_Pincher_ROS2/phantom_ws
```

Asegurarse de que Git LFS est√° activo para los archivos pesados (STL, videos, etc.). En caso de estarlo deben aparecer dentro de `phantom_ws/src/phantomx_pincher_description/meshes`

### 3.3 Compilaci√≥n del workspace

El workspace principal se encuentra en `phantom_ws/`.

```bash
cd phantom_ws
./build.sh
source install/setup.bash
```

El script `build.sh` fuerza el uso de `/usr/bin/python3` y configura `colcon` para evitar conflictos con entornos de Python externos.

### 3.4 Ejecuci√≥n b√°sica (robot real + visi√≥n + clasificador)

1. **Stack de movimiento (robot real)**

```bash
cd phantom_ws
source /opt/ros/humble/setup.bash   # o jazzy
source install/setup.bash

ros2 launch phantomx_pincher_bringup phantomx_pincher.launch.py \
  use_real_robot:=true \
  start_clasificador:=true
```

2. **Visi√≥n (c√°mara + YOLO)**

```bash
cd phantom_ws
source /opt/ros/humble/setup.bash   # o jazzy
source install/setup.bash

export PINCHER_YOLO_MODEL=$PWD/runs/classify/yolo_shapes_long/weights/best.pt

ros2 launch phantomx_pincher_bringup vision_bringup.launch.py \
  start_camera:=true \
  camera_device:=/dev/video2 \
  start_clasificador:=false
```

Con esta configuraci√≥n:

- MoveIt 2 y el nodo `commander` controlan el brazo real a trav√©s del controlador `follow_joint_trajectory`.
- `yolo_recognition_node` detecta las figuras en el ROI de la c√°mara.
- `clasificador_node` escucha el tipo de figura detectada y ejecuta la rutina completa de pick & place.


##PLANO DE PLANTA
![Plano](https://github.com/user-attachments/assets/3f22177a-4c98-4dbd-ae4e-058bb7476d0a)



## 4. Calibraci√≥n manual de las poses

Para definir las poses utilizadas en la rutina (zona de recolecci√≥n y canecas), se sigui√≥ el flujo de la gu√≠a de MoveIt:

1. **Lanzar el robot con MoveIt y RViz**
   
   ```bash
   cd phantom_ws
   source install/setup.bash
   ros2 launch phantomx_pincher_bringup phantomx_pincher.launch.py
   ```

   En RViz se selecciona se mueven las articulaciones una por una hasta una pose adecuada (por ejemplo, sobre la zona de recolecci√≥n o encima de una caneca).

2. **Leer la pose del efector final con `tf2_echo`**

   En otro terminal:

   ```bash
   source install/setup.bash
   ros2 run tf2_ros tf2_echo phantomx_pincher_base_link phantomx_pincher_end_effector
   ```

   Con este comando se obtienen en tiempo real:

   - La **traslaci√≥n** \(x, y, z\) del efector final.
   - La **orientaci√≥n** en cuaterniones (que luego se convierte a \(roll, pitch, yaw\) si es necesario), y estos valores junto con la traslaci√≥n se guardan para cada pose.

3. **Registrar poses clave**

   Para cada posici√≥n importante de la rutina se anotaron los valores devueltos por `tf2_echo`, por ejemplo:
   - Pose de **home**.
   - Pose de **aproximaci√≥n**
   - Pose de **recolecci√≥n**.
   - Pose sobre la **caneca roja**.
   - Pose sobre la **caneca verde**.
   - Pose sobre la **caneca azul**.
   - Pose sobre la **caneca amarilla**.

   Los valores de estas poses se pueden ver en `phantom_ws/src/phantomx_pincher_bringup/config/poses.yaml`

4. **Verificaci√≥n usando `/pose_command`**

   Despu√©s de guardar una pose, se cerr√≥ RViz, se relanz√≥ el bringup y se verific√≥ cada pose enviando un mensaje al t√≥pico `/pose_command` desde la terminal, por ejemplo:

   ```bash
   ros2 topic pub -1 /pose_command phantomx_pincher_interfaces/msg/PoseCommand \
   "{x: 0.128, y: 0.0, z: 0.100, roll: 3.142, pitch: 0.0, yaw: 0.0, cartesian_path: false}"
   ```

   De esta manera se ajustaron manualmente las poses hasta obtener movimientos suaves y seguros en el espacio de trabajo real.
   >Es importante en esta parte mencionar que en ocasiones MoveIt no logra ir a la pose seleccionada, para esto se puede quitar el par√°metro **-1** que manda un solo intento y en su lugar, al quitarlo, manda constantemente y al hacerlo, en alguno de los intentos lograr√° hallar la IK para la trayectoria para ir a la pose. Esto es un problema que se presenta en ROS Humble y que ya fue solucionado para ROS Jazzy.

## 5. Nodos de movimiento basados en `/pose_command` y MoveIt

La comunicaci√≥n de alto nivel para el movimiento del brazo se realiza mediante el t√≥pico:

```bash
/pose_command
```

con mensajes del tipo:

```text
phantomx_pincher_interfaces/msg/PoseCommand
```

### 5.1 Pruebas iniciales desde la terminal

Antes de crear nodos espec√≠ficos, se probaron las poses directamente desde la consola:

```bash
ros2 topic pub -1 /pose_command phantomx_pincher_interfaces/msg/PoseCommand \
"{x: X_OBJ, y: Y_OBJ, z: Z_OBJ, roll: R_OBJ, pitch: P_OBJ, yaw: Y_OBJ, cartesian_path: false}"
```

Usando las poses calibradas con `tf2_echo`, esto permiti√≥ verificar la cinem√°tica inversa y el correcto movimiento para cada pose.

### 5.2 Nodos creados para el movimiento

El sistema de movimiento del robot est√° organizado en varios paquetes ROS¬†2, cada uno con una funci√≥n espec√≠fica dentro del flujo de control y planeaci√≥n de trayectorias:

- **`phantomx_pincher_bringup`**: Es el encargado de lanzar el sistema completo. Aqu√≠ se incluye el lanzamiento de MoveIt 2, los nodos auxiliares y los controladores necesarios. Los controladores que se utilizan en este contexto son:
  - **`joint_state_broadcaster`**: Publica el estado actual (√°ngulos, velocidades) de cada articulaci√≥n.
  - **`joint_trajectory_controller`**: Recibe trayectorias calculadas por MoveIt y se encarga de enviar los comandos de posici√≥n/velocidad a los motores del robot, ya sea en simulaci√≥n o en hardware real.
  - Este paquete tambi√©n inicia el nodo `commander`, que es el responsable de coordinar la ejecuci√≥n de movimientos de alto nivel.

- **`phantomx_pincher_moveit_config`**: Contiene la configuraci√≥n detallada de MoveIt, que incluye:
  - Archivos SRDF con la descripci√≥n sem√°ntica del robot (grupos de articulaciones, cadenas cinem√°ticas).
  - Configuraci√≥n de los planificadores y algoritmos de movimiento (como OMPL).
  - Par√°metros para los controladores y l√≠mites articulares.
  Este paquete permite que MoveIt pueda planificar trayectorias seguras para el brazo.

- **`phantomx_pincher_description`**: Incluye el modelo del robot en formato URDF/Xacro, as√≠ como la descripci√≥n del entorno (base, canecas de colores, m√°stil, canastilla, etc.). Este modelo es esencial tanto para la simulaci√≥n, la visualizaci√≥n en RViz, como para el c√°lculo de trayectorias y colisiones en MoveIt.

- **`pincher_control`**: Este paquete implementa la l√≥gica de control de m√°s alto nivel. Aqu√≠ se desarrolla el nodo que se suscribe al t√≥pico `/pose_command`, interpreta el mensaje de pose deseada y ordena a MoveIt que planifique y ejecute el movimiento correspondiente.
  - En funci√≥n de la secuencia que se desee (por ejemplo: ir a la pose de recolecci√≥n, trasladar el objeto a una caneca, volver a home), este nodo publica sucesivamente comandos a MoveIt y verifica la ejecuci√≥n correcta.

De esta forma, el conjunto de paquetes y controladores permite que el usuario o una aplicaci√≥n externa le indique, mediante `/pose_command`, una posici√≥n y orientaci√≥n deseada para el efector final del robot, y el sistema se encargue autom√°ticamente de resolver la cinem√°tica, planificar la trayectoria y ejecutar el movimiento f√≠sico de forma segura, tanto en simulaci√≥n como sobre el hardware real.


- `phantomx_pincher_bringup`: lanza MoveIt 2, los controladores y el nodo `commander`.
- `phantomx_pincher_moveit_config`: contiene la configuraci√≥n de MoveIt (SRDF, planning groups, controladores, l√≠mites, etc.).
- `phantomx_pincher_description`: define el URDF/Xacro del robot y del entorno (base, canecas, m√°stil, canastilla).
- `pincher_control`: contiene nodos de m√°s alto nivel, entre ellos el nodo que escucha `/pose_command` y ejecuta la secuencia de movimiento.

El nodo de movimiento (en `pincher_control`) se encarga de:

1. **Suscribirse** al t√≥pico `/pose_command`.
2. Convertir el mensaje `PoseCommand` en una **pose objetivo** para el grupo de planificaci√≥n del brazo.
3. Usar la API de **MoveIt 2** para:
   - Resolver la **cinem√°tica inversa** (IK) de la pose objetivo.
   - Planear una trayectoria en el espacio de juntas.
4. Enviar la trayectoria resultante al controlador `follow_joint_trajectory` para su ejecuci√≥n.

### 5.3 ¬øC√≥mo planea MoveIt 2 las trayectorias?

De forma resumida:

- El nodo de movimiento fija una **pose objetivo** para el efector final (posici√≥n y orientaci√≥n).
- MoveIt utiliza un plugin de **cinem√°tica inversa** para convertir esa pose en una configuraci√≥n de articulaciones v√°lida.
- A partir de la posici√≥n actual y de la posici√≥n objetivo, MoveIt llama a un planificador (por ejemplo, de OMPL) que genera una **trayectoria continua** en el espacio de juntas respetando l√≠mites de velocidad, aceleraci√≥n y colisiones.
- La trayectoria generada se env√≠a al controlador `follow_joint_trajectory`, que interpola las referencias y mueve f√≠sicamente el brazo.

En la rutina final, el nodo de alto nivel concatena varias poses (recolecci√≥n, tr√°nsito, sobre caneca, dep√≥sito, regreso a reposo) publicando secuencialmente mensajes en `/pose_command`.

## 6. Visi√≥n por computador y YOLO

La parte de visi√≥n se basa en una c√°mara **ORBBEC Astra Pro** apuntando a la zona de recolecci√≥n. Con ella se construy√≥ un dataset y se entren√≥ un modelo de **YOLO** para clasificaci√≥n de figuras.

### 6.1 Adquisici√≥n y preparaci√≥n del dataset

- Se capturaron im√°genes reales en el laboratorio colocando las figuras (cubo, cilindro, pent√°gono, rect√°ngulo y fondo vac√≠o) dentro del **ROI** definido por la canastilla.
- Se tomaron muestras con diferentes posiciones y ligeras variaciones de iluminaci√≥n para mejorar la robustez del modelo.
- Las im√°genes se organizaron en carpetas por clase dentro de `dataset_yolo/`.
- El dataset se dividi√≥ en subconjuntos de **entrenamiento** y **validaci√≥n** en un 80/20.

### 6.2 Entrenamiento del modelo YOLO

El entrenamiento se realiza con el script `train_yolo.py` ubicado en el workspace:

- El script carga las im√°genes de `dataset_yolo`.
- Se configura el modelo YOLO de clasificaci√≥n con las 5 clases (cubo, cilindro, pent√°gono, rect√°ngulo, vac√≠o).
- Se entrena el modelo hasta converger en la m√©trica de validaci√≥n.
- El mejor modelo se guarda en `runs/classify/.../weights/best.pt`.

Este modelo se referencia posteriormente con la variable de entorno:

```bash
export PINCHER_YOLO_MODEL=$PWD/runs/classify/yolo_shapes_long/weights/best.pt
```

### 6.3 Inferencia en l√≠nea e integraci√≥n con el movimiento

Durante la ejecuci√≥n:

1. El nodo `vision_bringup` inicializa la **c√°mara ORBBEC Astra Pro** y publica im√°genes en un t√≥pico de imagen.
2. `yolo_recognition_node` recibe las im√°genes, recorta el **ROI** (zona de recolecci√≥n) y ejecuta inferencia con el modelo YOLO entrenado.
3. El nodo de visi√≥n publica el **tipo de figura detectada** en un t√≥pico de alto nivel (por ejemplo, `/figure_type`).
4. El nodo `clasificador_node` escucha este t√≥pico y, seg√∫n la figura detectada, decide qu√© secuencia de poses enviar por `/pose_command`:
   - Si la figura es **cubo** ‚Üí secuencia hacia la **caneca roja**.
   - Si es **cilindro** ‚Üí secuencia hacia la **caneca verde**.
   - Si es **pent√°gono** ‚Üí secuencia hacia la **caneca azul**.
   - Si es **rect√°ngulo** ‚Üí secuencia hacia la **caneca amarilla**.

De esta forma, la visi√≥n y el movimiento quedan integrados en una rutina completamente aut√≥noma de clasificaci√≥n de objetos.


## 7. Estructura de carpetas y paquetes principales

A nivel de repositorio:

- `phantom_ws/` ‚Äî Workspace de ROS 2 que contiene todos los paquetes del proyecto.
- `guias/` ‚Äî Documentaci√≥n adicional (MoveIt, Setup, Proyecto, etc.).
- `matlab/` ‚Äî Scripts auxiliares en MATLAB relacionados con el laboratorio.
- `docs_lab5/` ‚Äî Documentos de contexto del laboratorio.

Dentro de `phantom_ws/src/` los paquetes principales son:

- `phantomx_pincher_description` ‚Äî URDF/Xacro del robot y del entorno (base, canecas, m√°stil de c√°mara, canastilla, etc.).
- `phantomx_pincher_moveit_config` ‚Äî Configuraci√≥n de MoveIt 2 (SRDF, grupos de planificaci√≥n, controladores, par√°metros de planificaci√≥n y RViz).
- `phantomx_pincher_bringup` ‚Äî Archivos `launch` para levantar el robot en simulaci√≥n o con robot real (`phantomx_pincher.launch.py`, `vision_bringup.launch.py`).
- `pincher_control` ‚Äî Nodos de control de alto nivel, integraci√≥n con Dynamixel, nodos de visi√≥n y nodos de rutina de clasificaci√≥n.
- Scripts de entrenamiento de YOLO (como `train_yolo.py`) y carpetas de dataset (`dataset_yolo/`) y resultados (`runs/`).

Esta organizaci√≥n permite separar claramente:

- La **descripci√≥n del robot**.
- La **configuraci√≥n de MoveIt**.
- El **bringup** (c√≥mo se lanza todo el sistema).
- Los **nodos de control y de visi√≥n**.

## 8. Resultados

En esta secci√≥n se documentar√°n los resultados obtenidos con el sistema en simulaci√≥n y con el robot real.

- **Simulaci√≥n:** funcionamiento completo de la rutina de clasificaci√≥n en el entorno simulado, mostrando el movimiento del brazo entre zona de recolecci√≥n y canecas.
- **Robot real:** ejecuci√≥n de la rutina con la c√°mara ORBBEC Astra Pro, detecci√≥n de figuras por YOLO e interacci√≥n f√≠sica con las piezas.

Matr√≠z de confusi√≥n del modelo YOLO implementado
<p align="center">
  <img src="./phantom_ws/runs/classify/yolo_shapes_long/confusion_matrix_normalized.png" width="400">
</p>

**Enlaces a videos (YouTube):**

- Video de implementaci√≥n: _[enlace por agregar]_

> Nota: seg√∫n la r√∫brica, los videos deben iniciar con la introducci√≥n de Labsir.


## 9. Conclusiones

En este proyecto se integraron **cinem√°tica, planificaci√≥n de movimiento y visi√≥n por computador** en un mismo sistema rob√≥tico, logrando una rutina de clasificaci√≥n aut√≥noma sobre el robot Phantom X Pincher. La calibraci√≥n manual de poses con MoveIt y `tf2_echo` permiti√≥ obtener posiciones seguras y reproducibles para la zona de recolecci√≥n y las canecas, sirviendo como base para el dise√±o de las trayectorias.

El entrenamiento de un modelo YOLO con im√°genes reales de la c√°mara ORBBEC Astra Pro demostr√≥ la importancia de contar con un dataset bien curado y balanceado para obtener buenas tasas de acierto en la detecci√≥n. Finalmente, la integraci√≥n entre los nodos de visi√≥n y los nodos de movimiento a trav√©s de ROS 2 valid√≥ el uso de esta plataforma para desarrollar sistemas rob√≥ticos modulares y escalables.

> El grupo puede complementar esta secci√≥n con m√©tricas cuantitativas (tiempos de ejecuci√≥n, tasas de acierto, fallos observados) y reflexiones personales sobre el trabajo realizado.
